\documentclass[9pt,technote]{IEEEtran}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amstext}
\usepackage{amsthm}
\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays
\usepackage{booktabs}
\usepackage{setspace}
\usepackage{mathtools}
\usepackage{upgreek}
\usepackage{multicol,lipsum}
\usepackage{adjustbox}

\newcommand*{\captionsource}[2]{%
  \caption[{#1}]{%
    #1%
    \\\hspace{\linewidth}%
    \textbf{Source:} #2%
  }%
}


%\renewcommand*{\Rnfont}{\scshape}

% some very useful LaTeX packages include:

%\usepackage{cite}      % Written by Donald Arseneau
                        % V1.6 and later of IEEEtran pre-defines the format
                        % of the cite.sty package \cite{} output to follow
                        % that of IEEE. Loading the cite package will
                        % result in citation numbers being automatically
                        % sorted and properly "ranged". i.e.,
                        % [1], [9], [2], [7], [5], [6]
                        % (without using cite.sty)
                        % will become:
                        % [1], [2], [5]--[7], [9] (using cite.sty)
                        % cite.sty's \cite will automatically add leading
                        % space, if needed. Use cite.sty's noadjust option
                        % (cite.sty V3.8 and later) if you want to turn this
                        % off. cite.sty is already installed on most LaTeX
                        % systems. The latest version can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/cite/

\usepackage{graphicx}   % Written by David Carlisle and Sebastian Rahtz
                        % Required if you want graphics, photos, etc.
                        % graphicx.sty is already installed on most LaTeX
                        % systems. The latest version and documentation can
                        % be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/required/graphics/
                        % Another good source of documentation is "Using
                        % Imported Graphics in LaTeX2e" by Keith Reckdahl
                        % which can be found as esplatex.ps and epslatex.pdf
                        % at: http://www.ctan.org/tex-archive/info/

%\usepackage{psfrag}    % Written by Craig Barratt, Michael C. Grant,
                        % and David Carlisle
                        % This package allows you to substitute LaTeX
                        % commands for text in imported EPS graphic files.
                        % In this way, LaTeX symbols can be placed into
                        % graphics that have been generated by other
                        % applications. You must use latex->dvips->ps2pdf
                        % workflow (not direct pdf output from pdflatex) if
                        % you wish to use this capability because it works
                        % via some PostScript tricks. Alternatively, the
                        % graphics could be processed as separate files via
                        % psfrag and dvips, then converted to PDF for
                        % inclusion in the main file which uses pdflatex.
                        % Docs are in "The PSfrag System" by Michael C. Grant
                        % and David Carlisle. There is also some information
                        % about using psfrag in "Using Imported Graphics in
                        % LaTeX2e" by Keith Reckdahl which documents the
                        % graphicx package (see above). The psfrag package
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/psfrag/

%\usepackage{subfigure} % Written by Steven Douglas Cochran
                        % This package makes it easy to put subfigures
                        % in your figures. i.e., "figure 1a and 1b"
                        % Docs are in "Using Imported Graphics in LaTeX2e"
                        % by Keith Reckdahl which also documents the graphicx
                        % package (see above). subfigure.sty is already
                        % installed on most LaTeX systems. The latest version
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/subfigure/

\usepackage{url}        % Written by Donald Arseneau
                        % Provides better support for handling and breaking
                        % URLs. url.sty is already installed on most LaTeX
                        % systems. The latest version can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/other/misc/
                        % Read the url.sty source comments for usage information.

%\usepackage{stfloats}  % Written by Sigitas Tolusis
                        % Gives LaTeX2e the ability to do double column
                        % floats at the bottom of the page as well as the top.
                        % (e.g., "\begin{figure*}[!b]" is not normally
                        % possible in LaTeX2e). This is an invasive package
                        % which rewrites many portions of the LaTeX2e output
                        % routines. It may not work with other packages that
                        % modify the LaTeX2e output routine and/or with other
                        % versions of LaTeX. The latest version and
                        % documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/sttools/
                        % Documentation is contained in the stfloats.sty
                        % comments as well as in the presfull.pdf file.
                        % Do not use the stfloats baselinefloat ability as
                        % IEEE does not allow \baselineskip to stretch.
                        % Authors submitting work to the IEEE should note
                        % that IEEE rarely uses double column equations and
                        % that authors should try to avoid such use.
                        % Do not be tempted to use the cuted.sty or
                        % midfloat.sty package (by the same author) as IEEE
                        % does not format its papers in such ways.

\usepackage{amsmath}    % From the American Mathematical Society
                        % A popular package that provides many helpful commands
                        % for dealing with mathematics. Note that the AMSmath
                        % package sets \interdisplaylinepenalty to 10000 thus
                        % preventing page breaks from occurring within multiline
                        % equations. Use:
%\interdisplaylinepenalty=2500
                        % after loading amsmath to restore such page breaks
                        % as IEEEtran.cls normally does. amsmath.sty is already
                        % installed on most LaTeX systems. The latest version
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/



% Other popular packages for formatting tables and equations include:

%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty which improves the
% LaTeX2e array and tabular environments to provide better appearances and
% additional user controls. array.sty is already installed on most systems.
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/

% V1.6 of IEEEtran contains the IEEEeqnarray family of commands that can
% be used to generate multiline equations as well as matrices, tables, etc.

% Also of notable interest:
% Scott Pakin's eqparbox package for creating (automatically sized) equal
% width boxes. Available:
% http://www.ctan.org/tex-archive/macros/latex/contrib/supported/eqparbox/

% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.


% Your document starts here!
\begin{document}
% Define document title and author
	\title{Switchable  Constraints  for  Robust  Pose  Graph  SLAM }
	\author{Abdelaziz Ben Othman 
	\thanks{Advisor: Prof.Dr ~Marcus Baum and Shishan Yang, Lehrstuhl f\"ur Informatik mit Schwerpunkt Sensorik, University of Passau, WS 2017/2018.}}
	\markboth{Master-Seminar f\"ur Mustererkennung mit Sensordaten (Pattern Recognition in Sensor Data ) }{}
	\maketitle

% Write abstract here
\begin{abstract}
SLAM algorithms that can deduce a trustable and accurate mapping without being affected by the presence of outliers has recently attracted increasing attention.\\
The task of constructing the graph is delegated to the front-end that has access to some visual and inertial sensors.
In the other hand, the back-end of the system relies heavily on these information and any data association errors in the front-end level will lead to catastrophic implications on the resultant map.
Instead of focusing on another data fusion approach, the proposed solution focuses on building a robust back-end capable of detecting and rejecting outliers using switchable constraints during optimization.\\
\textbf{Keywords:} Self-Localization, loop closure, robust back-end 

\end{abstract}

% Each section begins with a \section{title} command
\section{Introduction}
	% \PARstart{}{} creates a tall first letter for this first paragraph
HUMANS invented robots to perform for them tasks that are dangerous or boring with both consistency and precision. By the time, robots evolved and are now doing more complex tasks (searching and rescuing people, exploring deep oceans, dunes on Mars etc.)\\
Such tasks are most often done in unknown environments where robots have to properly react to unknown occurrences.
% Explain SLAM and its problematic %
To do so, these intelligent machines have to reason about their environment by using the SLAM approach, which is an acronym for simultaneous localization and mapping.
The architecture of SLAM systems is composed by two main components: the front-end and the back-end \cite{sunderhauf2012switchable}.
The front-end collects information using sensors, performs data association, builds and maintains the graph representation of the SLAM problem while the back-end solves this problem by applying non-linear least squares equations.
While Simultaneous Localization and Mapping is in principle a solved problem, building a robust SLAM system is a challenging engineering problem and is still far from being an established and reliable technology.\\ 
This is noticed during what has become known as loop closing where the front-end has the full responsibility of implementing sophisticated algorithms that actively prevent the occurrence of data association or place recognition errors. 
In the other hand, the back-end relies heavily on the performance of the front-end and like all least squares problems solver, the presence of outliers like false positive loop closures will lead to a corrupted solution of the SLAM problem.
Although, the problem is acknowledged in literature, no concise solution has been proposed so far.\cite{kaess2011isam2}  \cite{kummerle2011g}
Since robustness is what prevents the robots from being applied outside controlled environments or specialized domains, enhancing the robustness of the SLAM is the scope of this paper.
In this approach, hidden variables known as the switchable variables are introduced to allow the back-end changing parts of the topological structure of the graph during the optimization process. The back-end can thereby discard loop closures and converge towards correct solutions even in the presence of false positive loop closures.\\
% Main Part
\section{SLAM as a Nonlinear Least Squares Optimization Problem}
After introducing the SLAM in general, we are going to concentrate now on a specific type of SLAM and show how it can be solved as a least squares optimization problem.
\subsection{Motion models}
Let's define $x_t$ as the robot's poses over discrete time steps $t$ and $M$ as the map the robot is discovering. The two terms are unknown a priori and have to be estimated using the odometry information that describe the movement between single poses.\\
The relation between two successive poses is given as:
\begin{equation}
\centerline{$x_{t+1} = f(x_t,u_t)+ w_t$ }  
\end{equation}
where $f$ is a non-linear motion model, $u_t$ is the odometry measurement between the poses and $w_t$ is the noise of the odometry sensor system.\\ The odometry noise follows a Gaussian distribution with a covariance matrix $\sum_{t}$:
\begin{equation}
\centerline{$w_t \sim \mathcal{N}(0,\sum_t) $}
\end{equation} 
as a result, $x_{t+1}$ follows as well a Gaussian distribution:
\begin{equation}
\centerline{$x_{t+1} \sim \mathcal{N}(f(x_t,u_t),\sum_t) $}
\end{equation}
\subsection{Sensor Models and Measurement Functions}
In addition to the odometry, the other sensor information are collected into a single variable vector and can be written as:
\newline 
\centerline{$Z_{t} = \{ z_0,z_1, ..., z_T\}$}
$T$ means we take care of all available sensor measurements in time.
We define $h$ as the sensor model function that allows us to predict the measurements given the current estimates of the map and the robot pose:
\begin{equation}
\centerline{$z_t = h(x_t,M) + \lambda_{t} $}
\end{equation}
where $\lambda_{t} $ captures the sensor noise which is modeled as a Gaussian distribution:
\begin{equation}
\centerline{$z_t \sim \mathcal{N}(h(x_t,M),\Lambda_{t}) $}
\end{equation}
\subsection{The pose graph SLAM problem}
The map we're interested in can be expressed as a graph where the vertices represent robot poses $x_i$ and edges represent the spatial constraints between those poses. Such a map is called a pose graph.
Both odometry and loop closure constraints are necessary for the pose graph 
SLAM.
The odometry constraint connects two successive states $x_{i}$ and $x_{i+1}$ via the f model such that: 
\begin{equation}
\centerline{$x_{i+1} \sim \mathcal{N}(f(x_i,u_i),\sum_i) $}
\end{equation}
To perform loop closing, the robot has to recognize places it already visited before. This is done in the front-end and introduces to us the second type of constraint, the loop closure constraints.
These constraints connect two not necessarily successive poses $x_i$ and $x_j$ such that: 
\begin{equation}
\centerline{$x_j \sim \mathcal{N}(f(x_{i},u_{ij}),\Lambda_{ij}) $}
\end{equation}
As the robot moves through its environment it uses the odometry sensors to create an initial guess of its trajectory. 
\subsection{Deriving a Nonlinear Least Squares Formulation}
The key solution of the full SLAM problem for the pose graph is estimating the posterior probability of the robot's trajectory X given all the measurements U.
\begin{equation}
\centerline { $P(X|U)$ }
\end{equation} 
Given the set of the odometry and loop closures constraints $u_i,u_{ij}\footnote{The displacement associated associated to a loop between two poses $x_{i}$ and $x_{j}$}\in U$, we seek the most likely configuration of robot poses and we denote it as $X^*$ where the distribution of $P(X|U)$ has its maximum. 
\begin{equation}
\centerline { $X^* = \underset{X}{\mathrm{argmax}} P(X|U)$}
\end{equation} 
In order to solve this problem, we can factor the joint probability distribution as: 
\begin{equation}
\centerline{$P(X|U) \propto \underbrace{\underset{i}\prod 
P(x_{i+1}|x_{i},u_{i})}_\text{Odometry Constraints}  \cdot \underbrace{ 
\underset{ij}\prod P(x_{j}|x_{i},u_{ij})}_\text{Loop Closure Constraints}$}
\end{equation} 
Under the assumption of the equations (6) and (7), the conditional probabilities above are all Gaussian. Thus we can write the odometry constraint as:
\begin{multline}
P(x_{i+1}|x_{i},u_{i}) = \frac{1}{\sqrt{2\pi |\sum_{i}|}} exp(-\frac{1}{2} (f(x_{i},u_{i})- x_{i+1})^{T} \\ \sum_{i}^{-1} (f(x_{i},u_{i})- x_{i+1} ))       
\end{multline}
The term \small{$\frac{1}{\sqrt{2\pi |\sum_{i}|}}$} is the normalizer constant and we replace it by $\eta$. We apply the  Mahalanobis
\footnote{
{\small  The squared Mahalanobis distance is defined as $\lVert a - b \rVert_{\sum_{i}}^{2}= (a-b)^T\sum^{-1}(a-b) $ } 
} 
distance definition to the term inside the exponent component in order to write the odometry constraint more conveniently:
\begin{equation}
\centerline { $P(x_{i+1}|x_{i},u_{i}) = \eta  exp(-\frac{1}{2}\lVert f(x_{i},u_{i}) - x_{i+1} \rVert_{\sum_{i}}^{2} ) $}
\end{equation}
By following the same steps for the loop closure constraints we gain:
\begin{equation}
\centerline { $P(x_{j}|x_{i},u_{ij}) = \eta  exp(-\frac{1}{2}\lVert f(x_{i},u_{ij}) - x_{j} \rVert_{\lambda_{ij}}^{2} ) $}
\end{equation}
This leads to the following factorization problem:
\begin{multline}
P(X|U) \propto \prod_{i} exp(-\frac{1}{2}\lVert f(x_{i},u_{i}) - x_{i+1} \rVert_{\sum_{i}}^{2} ) . \\  \prod_{ij} exp(-\frac{1}{2}\lVert f(x_{i},u_{ij}) - x_{j} \rVert_{\lambda_{ij}}^{2} )
\end{multline}
We transform the products into sum by taking the negative logarithm:
\begin{equation}
-\log{P(X|U)} \propto \sum_{i} \lVert f(x_{i},u_{i}) - x_{i+1} \rVert_{\sum_{i}}^{2}  + \sum_{ij} \lVert f(x_{i},u_{ij}) - x_{j} \rVert_{\lambda_{ij}}^{2} 
\end{equation}
Notice that due to our interest on the proportionality in this equation, we dropped the $\frac{1}{2}$ and the $\eta$ terms.
We can now solve the maximum a posteriori solution $X^*$:
\begin{multline}
X^* = \argmax_X P(X|U) = \underset{X}{\mathrm{argmin}} -\log P(X|U)  \\  
	= \underset{X}{\mathrm{argmin}} \underbrace{\sum_{i} \lVert f(x_{i},u_{i}) - x_{i+1} \rVert_{\sum_{i}}^{2}}_\text{odometry Constraints}  + \underbrace{\sum_{ij} \lVert f(x_{i},u_{ij}) - x_{j} \rVert_{\lambda_{ij}}^{2}}_\text{Loop Closure Constraints}
\end{multline}
This however is a least squares optimization problem, since the sought $X^{*}$ is a minimizer over a sum of squared terms.
\subsection{When Optimization fails}
When the robot is exploring the map, it relies on fixed landmarks observations to update its position. The front-end part associates to each known landmark some sensor measurements and to the unknown one new sensor measurements. However, taking into account the sensor reading errors and difficulties to decide on many known landmarks occurring near each other, the robot as consequence can't decide and makes inevitable data association errors. Such errors are known as false-positive loop closure: Due to the high self-similarity of many indoor and outdoor environments, two distinct places can actually appear to be very similar to the sensor (figure.1).
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.4\textwidth]{loop_closure.png}
  \captionsource{Images from different places: note the strong similarities between these images. }{\cite{WinNT}}
\end{figure}

\iffalse
Least squares problems can be mathematically solved by many algorithms. Previous researches have been done to use these methods in the context of SLAM and solve it in its least square formulation problem.
However, a major problem of these current optimization approaches arises from the strict division into a front-end and a back-end part.
We know that the front-end is responsible for sensor data processing, data association and graph construction.
In the other hand the back-end part relies on it heavily and considers the data association a solved problem.
Any errors occurring in the front-end will have catastrophic impact on the resultant map and robot state estimates. This is due to the nature of the back-end which a least square problem solver and thus vulnerable to outliers and errors.   
Typical data association errors happening in SLAM are false-positive closure constraints:  Due to the high self-similarity of many indoor and outdoor environments, two distinct places can actually appear to be very similar to the sensor.
\fi 
\section{A robust back-end for SLAM}
In previous section, we saw that false loop closure constraints are a sever problem. They corrupt the pose graph formulation of the SLAM problem with erroneous
edges, leading to a topologically incorrect graph. 
Our main idea is to increase the robustness of the back-end by rendering the topology of the graph subject to optimization instead of keeping it fixed.\\
Figure1 illustrates the main idea: If we identify and remove outliers during the optimization process, the graph topology would be corrected and the optimization could converge towards a correct solution.
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.4\textwidth]{slam1.png}
  \captionsource{In current state of the art approaches, only the robot poses $x_{i}$ are variables
in the optimization. We propose to augment the problem and also make the loop closure constraint edges subject to the optimization.}{\cite{sunderhauf2012robust}}
\end{figure}
\subsection{First step towards a Mathematical formulation }
Removing an edge from the graph corresponds to removing the constraint associated with that edge. A Loop closure constraint are expressed as we recall in (7).
We Disable a loop closure constraint on the graph by completely removing it from the formulation problem.  A binary weight $w_{ij}$ would allow us to do so: if $w_{ij} = 0$, the associated constraint is removed else we keep it.
Together with the odometry constraint we can write: 
\begin{multline}
X^* = \underset{X}{\mathrm{argmin}} \underbrace{\sum_{i} \lVert f(x_{i},u_{i}) - x_{i+1} \rVert_{\sum_{i}}^{2}}_\text{odometry Constraints}  + \\
\underbrace{\sum_{ij} \lVert w_{ij}.f(x_{i},u_{ij}) - x_{j} \rVert_{\lambda_{ij}}^{2}}_\text{Loop Closure Constraints}
\end{multline}
In next steps, we will make the weights also subject to the optimization instead of keeping them constant in order to achieve the desired behaviour: The
topology of the constraint graph is subject to the optimization process.
\subsection{The Switch Variables and Switch Function:}
For each weight $w_{ij}$, we introduce a continuous variable $s_{ij}$ and we call it switch variable.
We introduce the switch function $\Psi{ (s_{ij}) }$ such that: 
\begin{center}
$ w_{i,j} = \Psi{ (s_{ij}) } : \mathbb{R} \rightarrow \{0,1\}$
\end{center}
This function maps the continuous set of the switchable variable $s_{ij}$ to the desired weights $w_{ij} \in \{ 0,1 \}.$\\ 
The $s_{ij}$ would be then the variables in the  optimization problem.
Different switch functions can be defined e.g a step function or a sigmoid. However the sigmoid function shows an undesired behaviour where  the  gradient is  very  small  over  a  large  range  of  values and  where  the
iterative optimizer can become stuck.
More  elaborate experiments  showed  that  a  simple  linear function of the form: $w_{ij} = \Psi^{lin}(s_{ij}) = s_{ij}$ results  in  a  better  convergence
behaviour if we constrain $ 0 \leq s_{ij} \leq 1$ \\ 
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.4\textwidth]{slam2.png}
  \caption{Factor graph representation of the augmented pose graph SLAM problem after  introducing  the  switch  variables}
\end{figure}
\newline
We are now ready to augment the optimization problem and  introduce the new switch variables $S = \{s_{ij}\}$
\begin{multline}
S^*, X^* = 
\underset{X,S}{\mathrm{argmin}}
 \sum_{ij} \lVert \mathbf{f(x_i,u_{i}) - x_{i+1}}\rVert _{\sum_{i}} ^{2} + \\ 
\sum_{ij}\lVert \mathbf{\Psi(s_{ij})  (f(x_i,u_{ij}) - x_j)}\rVert _{\Lambda_{ij}} ^{2}     
\end{multline}
Notice now that the equations depends over two sets of hidden variables, the robot poses $X=\{ x_{ij} \}$ and the switch variable $S = \{s_{ij}\}$.

\subsection{The Switch Prior Constraint}
The switch variables  and like all other variables need to be initialized.
Keeping in mind that the loop closures are proposed in the front-end. Some of them are false positive and need to be identified and disabled.
Initially, we will accept all loop closures constraints, i.e.  letting $ w_{ij} = \Psi{s_{ij}} \approx 1 $.
For the following, we call these initial values $ \gamma_{ij}$  and collect them into the set $\Gamma = \{\gamma_{ij}\}$.
The switch variable $s_{ij}$ are modeled as normally distributed Gaussian variables\footnote{remember from equation (6) and (7)}.\\
\begin{center}
$ s_{ij}  \approx \mathcal{N}(\gamma_{ij}, \Xi_{ij} )$ 
\end{center}
Note that $\Xi_{ij}$ is the covariance matrix of the initial variables $\gamma_{ij}$.
The initial values $\gamma_{ij}$ will be part of the optimization problem using prior constraints.
These prior factors express the problem of maximizing
$P(S|\Gamma)$.
\begin{equation}
\centerline { $S^* = \underset{S}{\mathrm{argmin}} \sum_{ij} \lVert \gamma_{ij} - s_{ij} \rVert_{\Xi_{i}}^{2} $}
\end{equation} 
Obviously, we can say that the solution for this sub-problem is $s_{ij} = \gamma_{ij}$. However, the switch variables are involved in equation(17), so that on a global scale the optimal solution may be $s_{ij} \neq \gamma_{ij}$. 
\subsection{Putting it all together}
We add now the switch prior constraint to the problem formulation of (17) to obtain the final robust optimization problem for pose graph SLAM: 
\begin{multline}
S^*,X^* = 
\underset{X,S}{\mathrm{argmin}}
 \sum_{ij} \lVert \mathbf{f(x_i,u_{i}) - x_{i+1}}\rVert _{\sum_{i}} ^{2} + \\  
\sum_{ij}\lVert \mathbf{\Psi{(s_{ij})}(f(x_i,u_{ij}) - x_j)}\rVert _{\Lambda_{ij}} ^{2} \\ + \sum_{ij} \lVert \gamma_{ij} - s_{ij} \rVert_{\Xi_{ij}}^{2}      
\end{multline}
This is our proposed robust problem formulation for the pose graph SLAM. It is an optimization problem equation over two sets of hidden variables, the robot poses $X=\{x_{i}\}$ and the switch variable $S = \{s_{ij}\}$.
We have three type of constraints: the odometry constraints representing the pose-to-pose motion
information, the loop closure constraints which is a constraint between three variables ($x_{i},x_{j}$,$s_{ij}$) and weighted by the variable $w_{ij}$ and the switch prior constraints which penalizes the deactivation of loop closure constraints. 
\section{Preparing the evaluation}
The evaluation part will answer the following questions:
\begin{itemize}
\item How robust is the robust back-end?
\item What is the impact of the outlier loop closure constraints on the final error of the optimization?
\item What influence on the runtime behaviour has robust back-end?
\end{itemize}
\subsection{Errors metrics for SLAM}
While we can judge the performance by visual inspection of the resulting map, we prefer to rely more on quantitative errors metrics to make good decisions.
We used the root-mean square error (RMSE) and the relative pose error (RPE). Also, we used the precision-recall to judge how well the robust back-end distinguishes between the true and the false positive loop closures.
\subsection{Datasets for the evaluation}
\begin{table}[!hbt]
		% Center the table
		\centering
        % Title of the table
		\caption{The dataset used during the evaluation}
		\label{tab:simParameters}
		% Table itself: here we have two columns which are centered and have lines to the left, right and in the middle: |c|c|
       \scalebox{0.78}{
       \begin{tabular}{lllll}
\hline
dataset               & Synthetic/real & 2D/3D & Poses & Loop closures \\ \hline
Manhattan(original)   & Synthetic      & 2D    & 3500  & 2099          \\
Manhattan($g^2o$ version) & Synthetic      & 2D    & 3500  & 2099          \\
City 10000            & Synthetic      & 2D    & 10000 & 10688         \\
Sphere2500            & Synthetic      & 3D    & 2500  & 2450          \\
Intel                 & real           & 2D    & 943   & 894           \\ 
Parking Garage        & real           & 3D    & 1661  & 4615          \\ \hline
\end{tabular}}
\end{table}

	% If you have questions about how to write mathematical formulas in LaTeX, please read a LaTeX book or the 'Not So Short Introduction to LaTeX': tobi.oetiker.ch/lshort/lshort.pdf

We used six different datasets during the evaluation as shown in Table 1. The synthetic datasets are created from simulation, while the two real-world datasets have been recorded in a 2D (Intel) and in a 3D (Parking Garage) environment respectively. For evaluation purposes, different initialization of the same dataset were used to see the impact of the initial estimates on the overall behavior of the back-end system.\\
For the two latter real-world datasets, we used the estimation results for the outlier-free dataset as pseudo ground truth.
\subsection{General Methodology} 
Different methodologies were proposed to introduce how these constraints were added. In real life, these wrong imposed outliers are introduced by the front-end after failing to recognize a place in the map.
\subsection{Policies for Adding Outlier Loop Closure Constraints}
For the evaluation, false positive closure constraints were added between two robot poses $x_{i}$ and $x_{j}$ to the spoiled datasets. The indices i and j are determined using four different policies.
\begin{itemize}
\item Random constraints:
This method adds random loop closure between two randomly chosen vertices $x_{i}$ and $x_{j}$ following a uniform distribution over all available indices.\\
Most of the constraints that are created using this policy will span over large areas of the dataset since they connect two distant poses.
\item Local constraints:
Following this policy, the first pose is chosen randomly from all possible vertices. Then, the second vertex is chosen so that it's in the spatial vicinity of the first vertex.
This follows the fact that nearby poses are most likely to appear similar than distant poses do. Thus false place recognitions are more likely to be established
between these nearby poses.
\item Randomly Grouped Constraints:
The randomly grouped policy picks up first randomly two poses i and j and then adds 20 successive constraints between the poses with indices i...i+20 and j ...j+20. 
\item Locally Grouped Constraints:
This policy is a mixture between local and randomly grouped policies. 
The first index i is chosen randomly, the second vertex j has to be near the vicinity of i. Then 20 successive constraints
between the vertices with indices i... i+20 and j...j+20 are added.  
\end{itemize}

\section{Evaluation}
After discussing all the theoretical information,
we will evaluate in this section the proposed robust-back end approach.
\subsection{The influence of $\Xi_{ij}$ on the Estimation Results}
The proposed formula described in equation(19) include the switch prior constraints term $\lVert \mathbf{\gamma_{ij} - s_{ij}}\rVert _{\Xi_{ij}} ^{2}$. Although we can't mathematically deduce the value of the switch prior variances $\Xi_{ij}$, we can set it empirically. 
Therefore we will explore the influence of the switch prior variances on the estimated results. \\
$\Xi_{ij}$ controls the penalty the system gains when the front-end deactivates a loop closure between two poses.
By taking into consideration this value for each loop constraint, the front-end could express a degree of confidence about that particular loop closure. Thus, the front-end assign small $\Xi_{ij}$ to loop closures when it is very certain about and large $\Xi_{ij}$ to loop closure that appear more doubtful about.\\
In the extreme case, when the front-end found difficulties  to decide about certain loop closure it will assign them all the same value of $\Xi_{ij}$.
To show the influence of the $\Xi_{ij}$, we assume $\Xi_{ij}$ = $\zeta$ to all constraints.\\
\subsubsection{Methodology} To explain the influence of $\zeta$, we study the variance of the relative pose errors $RPE_{pos}$ and $RPE_{ori}$ for different values of random outliers (1, 10, and 100) and  $\zeta$ on the Manhattan dataset. Figure illustrates the results we have:
Every data point represents the mean $RPE_{pos}$  of 10 trials for a particular pairing of $\zeta$ and number of outliers. 
\begin{figure}
[h!]
\centering
\includegraphics[width=0.5\textwidth]{RPE_pos.png}
\end{figure}
\subsubsection{Results and Interpretation}
The plot in confirms that the value of $\Xi_{ij} = \zeta$ indeed influences the quality of the optimization result. We can see clearly that the quality of the estimation drops drastically if $\zeta$ is too large or small (0.3 $<$ $\zeta$ $>$ 2.0). The most important result is that the RPE stays relatively constant for values of $\zeta$ between 0.3 and 1.5 where the error is minimal. This result is independent from the number of outliers. 
\begin{itemize}
\item Result: If the front-end found difficulties to assign sound values to $\Xi_{ij}$, it is safe to set $\Xi_{ij} = 1$ since this value is close to the individual optimal choice independently of the number of outliers. 
\end{itemize}

\subsection{The Robustness in the presence of Outliers}
After choosing a sound value for $\Xi_{ij}$, we now examine how well the back-end performs in the presence of outliers.
The evaluation will compare the performance of the proposed robust back-end against those proposed in the state of the art in presence of outliers. We will furthermore see how much number of outliers can influence the estimation result. \\ 
\subsubsection{Methodology}
As we mentioned before, large number of datasets were considered during the evaluation. We tunned the number of added outliers (between 0 and 1000) using the four policies described above. For each  number of additional wrong outliers, 10 trials per policy were calculated, resulting in a total of 500 trials per dataset. For each trial, the error metrics $RPE$ and $RMSE$ were calculated. Notice from table that the number of correct loop closure constraints in the datasets varied between 10688 (City10000) and only 894 (Intel).
Therefore, adding extra 1000 loop closures will increase the loop closure ratio (112\% for the Intel dataset). While in real life applications we expect much less loop closure ratio, we will keep this high ratio in our experiments to see how robust the designed system is.\\ 
\subsubsection{Results and Interpretation}
Table \ref{my-label} summarizes the results we got.
Different measurements are mentioned here as : The minimum, maximum and median $RPE_{pos}$, as well as a success rate which measures the percentage of correct solutions.   
From table . we can see also that except for the parking garage dataset, the overall success rate are very high.
In total, from all 2500 trials, only two failed, leading to a success rates $ \simeq 100\%$.
We mention here that the two failure cases (The Manhattan and the Sphere world datasets) would be successfully implemented if the Huber function was used in combination with the implemented robust back-end.\\
The Parking garage dataset is the exception from the successful implementation. We will discuss in details the reasons for this unexpected exception.

\begin{itemize}
\item Result: The proposed back-end was able to solve 2498 out of 2500 trials on different datasets with up to 1000 outlier constraints. In combination with the Huber Cost function, all the 2500 trials were successfully solved leading to a success rate of 100\%.
\end{itemize}
\begin{table}[h]
\centering

\caption{Overall $RPE_{pos}$ metric for the different datasets, with...1000 outliers and 500 trials per dataset.}
\label{my-label}
\adjustbox{width=0.5\textwidth,height=1.3cm}{
\begin{tabular}{@{}lllllll@{}}
\toprule
Dataset              & max outl. ratio & min $RPE_{pos}$ & max $RPE_{pos}$ & median $RPE_{pos}$ & incorrect solutions & success rate \\ \midrule
Manhattan ($g^2{o}$) & 47.6\%          & 0.0009          & 0.0009          & 0.0009             & 0                   & 100\%        \\
Manhattan (orig.)    & 47.6\%          & 0.0009          & 5.9659          & 0.0009             & 1                   & 99.8\%       \\
City 10000           & 9.4\%           & 0.0005          & 0.0005          & 0.0005             & 0                   & 100\%        \\
Sphere2500           & 40.8\%          & 0.0953          & 0.0953          & 0.0964             & 1                   & 99.8\%       \\
Intel                & 111.9\%         & 0.2122          & 0.2122          & 0.2132             & 0                   & 100\%        \\ \bottomrule
\end{tabular}}
\end{table}


\begin{figure}
[h!]
\centering
\includegraphics[width=0.5\textwidth]{RPE_outliers.png}
\captionsource{Comparison of RPE measures between the proposed robust (solid line) and the state of the art non-robust back-ends (dashed).
Notice how the robust solution is up to two orders of magnitude more  accurate  for large  numbers  of  outliers  and  stays  constant, independently of the amount of outliers.}{\cite{sunderhauf2012switchable}}

\end{figure}
\begin{figure}
[h!]
\centering
\includegraphics[width=0.5\textwidth]{precision_recall.png}
\captionsource{Precision-recall statistics for the various datasets. Notice the scale of the X-axis (recall). The results indicate a close to optimal performance of the proposed system.}{\cite{sunderhauf2012switchable}}
\end{figure}
We had two quantitative performance metrics to evaluate the robustness of the proposed method : 
\begin{itemize}
\item RPE 
\item precision recall
\end{itemize}
While RPE compares the deviation of the estimated trajectory from the ground truth, the precision-recall allow us to determine how well the back-end performs i.e. how well it can identify, eliminate false loop closures and keep the true ones intact.\\ 
Optimality is reached when the system have precision-recall equal to 1, by deactivating all false positives and keeping true positives ones untouched. 
However, we mention that the back-end never made a binary decision on whether a constraint is supposed to be active or deactivated. It is only the precision-recall benchmark, that emulates such a behaviour. 
\\ From (figure4) we can deduce that the back-end reached optimal performance for the Intel, City10000, and Manhattan$( g^2{o})$ datasets, the recall is exactly 1 for a large span of precision. For sphere2500 and Olson’s version of the Manhattan world datasets, the recall is slightly smaller, due to the two failure cases.
\begin{itemize}
\item Result: The proposed back-end reaches almost optimal results in terms of precision-recall. This confirms that all false positives are identified and eliminated while almost all true positive closure constraint are left intact.
\end{itemize} 
\subsection{Convergence and Runtime Behaviour}
We expect the convergence time to increase with the number of the added outliers to the datasets. \\ 
\subsubsection{Methodology}
The datasets used in previous section were spoiled by 0 to 1000 outliers using all four policies. The time of convergence was used on a Core2-Duo desktop machine running at 2.4 Ghz.
To inspect the convergence behaviour, 10 trials with 1000 outliers were selected from each dataset and each outlier policy. We define $\chi^2$ to be the error measure the optimizer tries to minimize.
Both $\chi^2$ and time of convergence were recorded and normalized during the optimization so that their final value tend to a value of 1. During this investigation, the final values we got were averaged over the 10 trials belonging to the same dataset and same outlier policy.\\
\subsubsection{Interpretation}
The results we had are very different and rely too much to the policy used to add the outliers to the datasets.
For the two non-local policies, the convergence time increases with the number of outliers while for the local policies, the convergence time increases slower.
Obviously the non-local outlier constraints that often connect two very distant places in the dataset require more time to be resolved.
\begin{itemize}
\item 
Result: The structure of the dataset, number of outliers and the applied policy are parameters on which the convergence time depend heavily. 
Connecting local poses of the robot result in faster convergence than constraints connecting distant places on the dataset.
\end{itemize}
\begin{figure}
[h!]
\centering
\includegraphics[width=0.5\textwidth]{failures.png}
\captionsource{Two failure cases for the Sphere World dataset in (a) and the Manhattan dataset (Olson’s original). Despite the failure cases, the maps are still locally consistent. Adding a Huber cost function to the robust back-end can resolve both cases.}{\cite{sunderhauf2012switchable}}
\end{figure}

\subsection{Discussion of the Failure Cases}
\begin{figure}
[h!]
\centering
\includegraphics[width=0.5\textwidth]{convergence_time.png}
\captionsource{Convergence time for different outlier policies for the different datasets.  Notice that the two local policies require much less time for convergence than the non-local policies.}{\cite{sunderhauf2012switchable}}
\end{figure}
\subsubsection{Manhattan and Sphere Datasets}

Two trials in the Sphere2500 and the Olson’s version of the Manhattan datasets failed to converge to a correct solution (see Figure 5). Although the resulting maps are significantly distorted when compared to the ground truth on a global level, they are still locally intact.
For the sphere world dataset, the resulting map consists of two individually consistent and intact sub-maps that are however misaligned to each other.
For the Manhattan dataset, a false positive loop closure constraint was not detected in the center of the map which is the reason of the failure. 
\begin{itemize}
\item Result: Even in case of failures, the proposed robust back-end degrades gracefully. Apparently not deactivating single false positives correctly leads to punctual  errors  that  cause  global distortion but retain local consistency.
\end{itemize}
\subsubsection{The garage Dataset}
Here, the robust back-end approach didn't perform better performance than the non-robust approach.\\
Figure.7 shows the ground truth trajectory and illustrates the results we had using the robust back-end technique.
Here we scaled the z-axis so the data could be seen in the context of its spatial structure.\\
The dataset contains four parking decks connected by only two strands of odometry constraints that originate from the driveways.
The problem arising from this sparse connection structure can be seen in Fig.7(b).
The insufficient amount of information on the relative pose of the individual decks leads to a small number of constraints (compared to previous datasets) which as a consequence leads to a failure. In simple words, we don't have enough odometry constraints that could vote against a false loop closure request.\\
Since SLAM systems has no knowledge about the structure of the environment, the proposed back-end fails to eliminate false positive loop closures.
\begin{itemize}
\item Results: The robust back-end solution fails to eliminate false positive loops in environments composed by sparsely connected parts. 
\end{itemize}    

\begin{figure}
[h!]
\centering
\includegraphics[width=0.5\textwidth]{parking_garage.png}
\captionsource{(a) shows the ground
truth trajectory from the side, (b) illustrates a failure exemplary produced by the robust back-end approach.   
}{\cite{sunderhauf2012switchable}}
\end{figure}


\section{Conclusion}
In this seminar paper, we proposed and evaluated a method to identify and reject outliers in  the  back-end  of  a  SLAM  system  by  using  switchable constraints.
The  feasibility  of  the  proposed  approach  has been demonstrated in a variety of  2D and 3D datasets that contain false positive loop closure constraints.
The proposed extension to the back-end part correctly identifies and disables false positive loop closure
constraints, while maintaining the correct (true positive) constraints. Even when it fails, the system degrades gracefully and the resulting map retains its local consistency. \\
We could increase the robustness of the proposed system by combining it with other measures such as the Huber cost function that decreases the influence of outliers.\\
Also choosing a suitable front-end to the overall SLAM system increases the robustness against errors in the place recognition.\\
The proposed approach is extensible and can be also applied to other domains where least squares problems have to be solved like multi-path mitigation in  GNSS-based localization .
\cite{sunderhauf2012switchable}
% Now we need a bibliography:
\bibliographystyle{plain}
\bibliography{references}


\end{document}